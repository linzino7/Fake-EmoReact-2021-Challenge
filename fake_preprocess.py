# -*- coding: utf-8 -*-
"""NLP_BERT_on_fake_news_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iyNDJE9Q59OUihSeee1wts0d1RBtYtI-

PyTorch version |	torchtext version
1.6	0.7

"""

#from google.colab import drive
#drive.mount('/content/drive')

"""# preprocess"""

import json
import pandas as pd
import numpy as np
import re


'''read data'''
path = './pubilc_data_0522'
train_str = ''

with open(path+'/train.json','r',encoding = 'utf-8')as f:
    train_str = f.read()
    #print(train_str)
    
f.close()

train = json.loads(train_str)


''' filter '''

#hashmap = {} # save item which is unique.
data = []

for dic in train:
    #if not dic['idx'] in hashmap:
      text = dic['text']+' '+dic['reply']
      #replace tag user
      #httpre = r'https:\/\/?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b\/?[-a-zA-Z0-9@:%._\+~#=]{1,256}'
      #text = re.sub(httpre, " ", text)
      #replace tag user
      #tagere = '@[-a-zA-Z0-9:%._\+~#=]{1,256}'
      #text = re.sub(tagere, "taguser", text)
      # leave a-z and space
      #text = re.sub(r'[^A-Za-z\s]', " ", text)

      
      label = 0
      if dic['label'] =='fake': # real
          label = 1
          
      
      # hashmap[dic['idx']] = {'idx':dic['idx'],
      #               'context_idx':dic['context_idx'],
      #                         'text':text,
      #                         'label':label}
      data.append( {'idx':dic['idx'],
                    'context_idx':dic['context_idx'],
                              'text':text,
                              'label':label})

#data = list(hashmap.values())
df = pd.DataFrame(data)

df

#raw_data_path = '/content/drive/My Drive/transformers/Data/news.csv'
destination_folder = './pubilc_data_0522'

train_test_ratio = 0.95
train_valid_ratio = 0.945

first_n_words = 512

import pandas as pd
from sklearn.model_selection import train_test_split

def trim_string(x):

    x = x.split(maxsplit=first_n_words)
    x = ' '.join(x[:first_n_words])

    return x

# Read raw data
df_raw = df

# Prepare columns
# df_raw['label'] = (df_raw['label'] == 'FAKE').astype('int')
# df_raw['titletext'] = df_raw['title'] + ". " + df_raw['text']
# df_raw = df_raw.reindex(columns=['label', 'title', 'text', 'titletext'])

# Drop rows with empty text
df_raw.drop( df_raw[df_raw.text.str.len() < 5].index, inplace=True)

# Trim text and titletext to first_n_words
#df_raw['text'] = df_raw['text'].apply(trim_string)

# Split according to label
df_real = df_raw[df_raw['label'] == 0]
df_fake = df_raw[df_raw['label'] == 1]

# Train-test split
df_real_full_train, df_real_test = train_test_split(df_real, train_size = train_test_ratio, random_state = 1)
df_fake_full_train, df_fake_test = train_test_split(df_fake, train_size = train_test_ratio, random_state = 1)

# Train-valid split
df_real_train, df_real_valid = train_test_split(df_real_full_train, train_size = train_valid_ratio, random_state = 1)
df_fake_train, df_fake_valid = train_test_split(df_fake_full_train, train_size = train_valid_ratio, random_state = 1)

# Concatenate splits of different labels
df_train = pd.concat([df_real_train, df_fake_train], ignore_index=True, sort=False)
df_valid = pd.concat([df_real_valid, df_fake_valid], ignore_index=True, sort=False)
df_test = pd.concat([df_real_test, df_fake_test], ignore_index=True, sort=False)

print('\t real \t fake')
print('train',len(df_real_train),len(df_fake_train))
print('valid',len(df_real_valid),len(df_fake_valid))
print('test',len(df_real_test),len(df_fake_test))

# Write preprocessed data
df_train.to_csv(destination_folder + '/train_mr.csv', index=False)
df_valid.to_csv(destination_folder + '/valid_mr.csv', index=False)
df_test.to_csv(destination_folder + '/test_mr.csv', index=False)

